"""
Validation metrics - Accuracy, Area, and Standard Error

These functions were derived from spreadsheet calculations generated by
S. Stehman and B. Pengra. They assume a particular structure to the input
data (in terms of map/reference axes).

Authors: D. Wellington
"""

import numpy as np


def _divide_with_0s(a, b):
    with np.errstate(divide='ignore', invalid='ignore'):
        result = np.array(a / b)
        result[np.isinf(result)] = np.nan
        return result


def producers_accuracy(error_matrix):
    """
    Calculate the producer's accuracy (1 - omission error) from an nxn
    confusion matrix. If the reference data contain no instances of a class,
    returns a NaN for that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: A 1d float array of size n with the producer's accuracy
    """

    totals = error_matrix.sum(axis=0)

    return _divide_with_0s(error_matrix.diagonal(), totals)


def producers_standard_error(error_matrix):
    """
    Calculate the producer's standard error from an nxn confusion matrix.
    If the reference data contain no instances of a class, returns a NaN for
    that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: A 1d float array of size n with the producer's standard error
    """

    totals = error_matrix.sum(axis=0)

    accuracy = producers_accuracy(error_matrix)

    return np.sqrt(_divide_with_0s(
            accuracy * (1 - accuracy),
            np.where(totals != 1, totals - 1, totals)))


def users_accuracy(error_matrix):
    """
    Calculates the user's accuracy (1 - commission error) from an nxn
    confusion matrix. If the reference data contain no instances of a class,
    returns a NaN for that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: A 1d float array of size n with the user's accuracy
    """

    return producers_accuracy(error_matrix.transpose())


def users_standard_error(error_matrix):
    """
    Calculate the user's standard error from an nxn confusion matrix.
    If the reference data contain no instances of a class, returns a NaN for
    that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: A 1d float array of size n with the user's standard error
    """

    return producers_standard_error(error_matrix.transpose())


def overall_accuracy(error_matrix):
    """
    Calculates overall accuracy from the diagonal of an error matrix, relative
    to the total number of values.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: Returns a float value for the overall accuracy.
    """

    return error_matrix.diagonal().sum() / error_matrix.sum()


def poststratified_producers_accuracy(error_matrix, wh):
    """
    Calculate the post-stratified producer's accuracy from an nxn
    confusion matrix. If the reference data contain no instances of a class,
    returns a NaN for that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A 1d float array of size n with the post-stratified producer's accuracy
    """

    calc_matrix = error_matrix * _divide_with_0s(wh, error_matrix.sum(axis=1))[:, None]

    totals = np.nansum(calc_matrix, axis=0)

    return _divide_with_0s(calc_matrix.diagonal(), totals)


def poststratified_producers_standard_error(error_matrix, wh):
    """
    Calculate the post-stratified producer's standard error from an nxn
    confusion matrix. If the reference data contain no instances of a class,
    returns a NaN for that element.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A 1d float array of size n with the post-stratified producer's
    standard error
    """

    totals = error_matrix.sum(axis=1)

    # Handle the n_ix matrix
    n_ix_weights = _divide_with_0s(
        (wh ** 2),
        (totals * np.where(totals != 1, totals - 1, totals)))
    n_ix = n_ix_weights[:, None] * error_matrix * (1-_divide_with_0s(error_matrix, totals[:, None]))
    d_i = np.diag_indices(error_matrix.shape[0])
    n_ix[d_i] = 0  # Zero the diagonal

    # Handle n_jx matrix
    n_jx = _divide_with_0s(error_matrix * wh[:, None], totals[:, None])

    p_acc = poststratified_producers_accuracy(error_matrix, wh)
    u_acc = users_accuracy(error_matrix)

    a = _divide_with_0s(
        (wh ** 2) * ((1 - p_acc) ** 2) * u_acc * (1 - u_acc),
        np.where(totals != 1, totals - 1, totals))

    return np.sqrt(a + (p_acc ** 2) * np.nansum(n_ix, axis=0)) / np.nansum(n_jx, axis=0)


def poststratified_dice_coefficients(error_matrix):
    """
    Calculates the post-stratified overall accuracy per class (dice coefficients).
    :param error_matrix: An nxn array for the error (or confusion) matrix
    :return: Returns a 1d array of size n with the dice coefficients.
    """

    return _divide_with_0s(error_matrix.diagonal()*2, (error_matrix.sum(axis=0) + error_matrix.sum(axis=1)))


def poststratified_producers_accuracy_overall(error_matrix, wh):
    """
    Calculate the overall post-stratified producer's accuracy from an nxn
    confusion matrix.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A single float value for the overall post-stratified producer's accuracy
    """

    totals = error_matrix.sum(axis=1)

    return np.nansum(_divide_with_0s(error_matrix.diagonal() * wh, totals))


def poststratified_producers_standard_error_overall(error_matrix, wh):
    """
    Calculate the overall post-stratified producer's standard error from an nxn
    confusion matrix.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A single float value for the overall post-stratified producer's
    standard error
    """

    return np.sqrt(np.nansum((users_standard_error(error_matrix)**2) * (wh**2)))


def _area_estimate_matrix(error_matrix, wh):

    totals = error_matrix.sum(axis=1)

    return _divide_with_0s(error_matrix * wh[:, None], totals[:, None])


def area_proportion(error_matrix, wh):
    """
    Calculates the estimated map area proportion of each class.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A 1d float array of size n with the estimated map area proportion
    by class.
    """

    return np.nansum(_area_estimate_matrix(error_matrix, wh), axis=0)


def area_proportion_standard_error(error_matrix, wh):
    """
    Calculates the estimated map area proportion standard error of each class.

    :param error_matrix: An nxn array for the error (or confusion) matrix
    :param wh: A 1d array of size n with the proportion of the map
    occupied by each particular class.
    :return: A 1d float array of size n with the estimated map area proportion
    standard error by class.
    """

    area_estimate = _area_estimate_matrix(error_matrix, wh)

    totals = error_matrix.sum(axis=1)

    calc_matrix = _divide_with_0s(
        (wh[:, None]**2) * area_estimate * (1 - area_estimate),
        np.where(totals != 1, totals - 1, totals)[:, None])

    return np.sqrt(np.nansum(calc_matrix, axis=0))


def get_error_matrix(data, years, axes, categories, year_header=None):
    """
    This function returns the error matrix for a provided data frame.

    :param data: Pandas data frame.
    :param years: A list of years for which to filter the data.
    :param axes: A two-element list with the names of the dataframe columns to use for the error matrix.
    :param categories: A list of category names (ensures pandas doesn't leave out any not represented in the data)
    :param year_header: String; the name of the column containing the year in the dataframe
    :return: Returns a NumPy 2d array
    """

    data_mask = data[year_header].isin(years)

    pv = data[data_mask].pivot_table(index=axes[0], columns=axes[1], aggfunc='size', fill_value=0)
    return pv.reindex(index=categories, columns=categories, fill_value=0).values


def class_proportions(histogram, years, columns, year_header=None):
    """
    Returns class proportions from histogram data.

    :param histogram: Histogram data, containing the class counts in the columns specified below
    :param years: A list of years for which to filter the data. For one year, provide a single-element list
    :param columns: A list of the names of the columns to use to build the histogram
    :param year_header: String; the name of the column containing the year in the dataframe
    :return: Returns a 1d NumPy array with class proportions.
    """

    wh = None

    if histogram is not None:
        histogram_mask = histogram[year_header].isin(years)
        wh = (histogram[histogram_mask].loc[:, columns].sum() /
              histogram[histogram_mask].loc[:, columns].sum().sum()).values
    return wh


def statistics(data, years, axes, categories, histogram_columns=None, histogram=None, data_year_header='image_year',
               histogram_year_header='year'):
    """
    Return all the statistical metrics in a dictionary.

    :param data: pandas dataframe with map and reference data
    :param years: A list of years for which to filter the data. For one year, provide a single-element list
    :param axes: A list of the names of the dataframe columns to use for the error matrix
    :param categories: A list of the categories (e.g., land cover codes) for the error matrix
    :param histogram_columns: A list of the names of the columns to use to build the histogram
    :param histogram: pandas dataframe containing histogram map data
    :param data_year_header: String; the name of the column containing the year in the map and reference dataframe
    :param histogram_year_header: String; the name of the column containing the year in the histogram dataframe
    :return: A dictionary with all the statistical calculations
    """

    error_matrix = get_error_matrix(data, years, axes, categories, year_header=data_year_header)
    wh = class_proportions(histogram, years, histogram_columns, year_header=histogram_year_header)

    stats = {
        'error_matrix': error_matrix,
        'users_accuracy': users_accuracy(error_matrix),
        'users_standard_error': users_standard_error(error_matrix),
        'producers_accuracy': producers_accuracy(error_matrix),
        'producers_standard_error': producers_standard_error(error_matrix),
        'overall_accuracy': overall_accuracy(error_matrix),
        'reference_totals': error_matrix.sum(axis=0),
        'reference_proportions': error_matrix.sum(axis=0) / error_matrix.sum(),
        'map_totals': error_matrix.sum(axis=1),
        'class_proportions': error_matrix.sum(axis=1) / error_matrix.sum(),
        'grand_total': error_matrix.sum()
    }

    if wh is not None:
        stats.update({
            'wh': wh,
            'poststratified_producers_accuracy':
                poststratified_producers_accuracy(error_matrix, wh),
            'poststratified_producers_standard_error':
                poststratified_producers_standard_error(error_matrix, wh),
            'poststratified_producers_accuracy_overall':
                poststratified_producers_accuracy_overall(error_matrix, wh),
            'poststratified_producers_standard_error_overall':
                poststratified_producers_standard_error_overall(error_matrix, wh),
            'poststratified_producers_accuracy_overall_upper':
                poststratified_producers_accuracy_overall(error_matrix, wh) +
                poststratified_producers_standard_error_overall(error_matrix, wh),
            'poststratified_producers_accuracy_overall_lower':
                poststratified_producers_accuracy_overall(error_matrix, wh) -
                poststratified_producers_standard_error_overall(error_matrix, wh),
            'poststratified_dice_coefficients':
                poststratified_dice_coefficients(error_matrix),
            'area_proportion':
                area_proportion(error_matrix, wh),
            'area_proportion_standard_error':
                area_proportion_standard_error(error_matrix, wh),
        })

    return stats


def change_nochange(reference_dataframe, allow_offset=0):
    """
    This function takes a Pandas dataframe and adds/edits columns to align land cover changes between years.

    The change mapping is performed as follows: for every change detected in the map data, a window is defined that
    encompasses a span of the dataset that contains any potentially relevant reference changes, as well as other map
    changes that may be potential matches within that window, depending on the allowed offset. The total number of
    valid matches within this window is determined for each shift from -allow_offset to +allow_offset,
    and the shifts are ranked depending on how many matches they produce if each was applied first (i.e., not
    considering every possible combination, only by how the top-level shift would rank).

    For shifts that produce the same number of matches, the lower absolute value of year shift is preferred; further,
    ranking between shifts that produce an equal number of changes in either direction favors the map being "slow",
    i.e., the negative shift value is ranked preferentially higher in the list. The actual nature of
    the change (forest to grass/shrub, e.g.) is not used to derive the shift ordering.

    :param reference_dataframe: A Pandas dataframe with the reference and associated map data for each plot and year.
        The dataframe must contain the following columns: "plotid", "image_year", "Reference", "LC_Primary".
    :param allow_offset: The maximum number of years that a change in the map class can be offset (either before or
        after the year the reference data changes) and be considered for a match. E.g., allow_offset=1 means the
        function will try to match changes for which the map data may be off by -1, 0, or +1 years.
    :return: A Pandas dataframe, with the following additional columns:
        Valid: True/False whether the datapoint is valid for change/no-change statistics (False entries are dropped)
        RefChg: 'Chg' or 'NoChg', whether or not a change occurred in the reference data for that plot/year
        MapChg: 'Chg' or 'NoChg', whether or not a change occurred in the map data AND is assigned to that plot/year
        MapChgYear: If MapChg = 'Chg', the year in the map data that the change occurred; 0 otherwise
        RefChgFromTo: Integer change code with from/to as the first/last digit, e.g. 103 = '1 to 3'. A code of 101,
            e.g., indicates no change (develeoped to developed)
        MapChgFromTo: Similar to the previous, but for changes in the map data
    """

    def changed(x, default=False, offset=0):
        if len(x) == 1:
            return default
        elif x[0] == (x[1]-offset):
            return False
        else:
            return True

    def valid_matches(df, shift, mask):
        return df.RefChg & \
               df.MapChg.shift(periods=shift, fill_value=False) & \
               mask & \
               mask.shift(periods=shift, fill_value=False)

    def get_change_window(series, index, offset):
        window = [index - offset, index + offset + 1]
        for w, s in zip([[0, 1], [1, 0]], [[0, offset], [offset, 0]]):
            slc0 = slice(*window)
            slc1 = slice(*[window[i] + s[i] for i in range(len(window))])
            while series[slc1].sum() > series[slc0].sum():
                window = [window[i] + w[i] for i in range(len(window))]
                slc0 = slice(*window)
                slc1 = slice(*[window[i] + s[i] for i in range(len(window))])
        return slice(*window)

    df = reference_dataframe.copy()
    df = df.sort_values(['plotid', 'image_year']).reset_index()

    # Rolling window to find changes in land cover class, plot id, or jumps in year
    ref_chg = df.Reference.rolling(2, min_periods=1).apply(
        lambda x: changed(x), raw=True).astype(np.bool)
    map_chg = df.LC_Primary.rolling(2, min_periods=1).apply(
        lambda x: changed(x), raw=True).astype(np.bool)
    plt_chg = df.plotid.rolling(2, min_periods=1).apply(
        lambda x: changed(x, default=True), raw=True).to_numpy(dtype=np.bool)
    year_chg_not_one = df.image_year.rolling(2, min_periods=1).apply(
        lambda x: changed(x, offset=1), raw=True).to_numpy(dtype=np.bool)

    # Potentially 'valid' data points for change/no-change are defined as follows:
    #  a) The 'plotid' did not change (the initial observations cannot be a change)
    #  b) The change in 'image_year' cannot be more than one (missing years are unknowns)
    #  c) The current and previous reference class cannot be a 0 (invalid value)

    df.loc[:, 'Valid'] = ~plt_chg & ~year_chg_not_one & ~(df.Reference.values == 0)
    df.loc[1:, 'Valid'] = df.Valid.values[1:] & ~(df.Reference.values[:-1] == 0)

    # ---- Initialize new columns ---- #

    df.loc[:, 'RefChg'] = ref_chg & df['Valid'].values  # Valid reference changes
    df.loc[:, 'MapChg'] = map_chg & df['Valid'].values  # Valid map changes, not shifted yet

    df.loc[:, 'MapChgYear'] = df['image_year'] * df['MapChg']  # Year of map change or zero

    # There will be some invalid entries here, but they will be filtered out later
    df['RefChgFromTo'] = (df.Reference.astype(np.int16) * 100) + df.Reference
    df.loc[1:, 'RefChgFromTo'] = (df.Reference[:-1].astype(np.int16).values * 100) + df.Reference[1:].values
    df['MapChgFromTo'] = (df.LC_Primary.astype(np.int16) * 100) + df.LC_Primary
    df.loc[1:, 'MapChgFromTo'] = (df.LC_Primary[:-1].astype(np.int16).values * 100) + df.LC_Primary[1:].values

    mutable = df.Valid.copy()  # Track which things are OK to change

    # ---- End of initialization ---- #

    # Find map changes that can be matched to those in the reference data set in other years, within tolerance
    if allow_offset:
        print('Adjusting changes...')
        change_indices = df[df.MapChg.values].index
        for change_index in change_indices:
            mask = df.plotid == df.loc[change_index, 'plotid']  # Only consider the same plotid
            change_compare = []
            window = get_change_window(df.MapChg | df.RefChg, change_index, allow_offset)
            for shift in range(-allow_offset, allow_offset + 1):
                change_compare.append((valid_matches(df, shift, mutable & mask)[window].sum(), shift))
            # Sort by decreasing total matches, then increasing shift amount
            change_compare.sort(key=lambda x: (-x[0], abs(x[1])))
            for changes in change_compare:
                n_changes, offset = changes
                if n_changes:
                    matches = valid_matches(df, offset, mutable & mask)
                    # Shift will only affect valid matches, or where the valid matches started from, for that window
                    shift_mask = (matches | matches.shift(periods=-offset, fill_value=False)) & \
                        df.index.isin(df[window].index)
                    # Update MapChg, MapChgYear, MapChgFromTo
                    df.loc[shift_mask, 'MapChg'] = \
                        (df.MapChg & shift_mask).shift(
                            periods=offset, fill_value=False)[shift_mask].values
                    df.loc[shift_mask, 'MapChgYear'] = \
                        (df.MapChgYear * shift_mask.astype(np.int16)).shift(
                            periods=offset, fill_value=0)[shift_mask].values
                    df.loc[shift_mask, 'MapChgFromTo'] = \
                        (df.MapChgFromTo * shift_mask.astype(np.int16)).shift(
                            periods=offset, fill_value=101)[shift_mask].values
                    # These matches will not be changed again
                    mutable[matches & df.index.isin(df[window].index)] = False

        # Fixing the change codes after moving stuff around above
        print('Adjusting change codes...')
        for i in df[df.MapChg.values].index:
            need_new_lc = True
            new_lc = 0
            for j in range(i, max(df.index) + 1):
                if plt_chg[j]:
                    break
                # If we've just jumped years, we don't know the LC
                if year_chg_not_one[j]:
                    need_new_lc = True
                # If we need LC, take it from LC_Primary if nonzero
                if need_new_lc and df.loc[j, 'LC_Primary']:
                    new_lc = df.loc[j, 'LC_Primary']
                    need_new_lc = False
                # If there's been a change, take the new LC from the change code
                if df.loc[j, 'MapChg']:
                    new_lc = df.loc[j, 'MapChgFromTo'] % 10
                    need_new_lc = False
                # Update non-change locations with LC code if possible.
                if (not need_new_lc) and (not df.loc[j, 'MapChg']) and (df.loc[j, 'LC_Primary']):
                    df.loc[j, 'MapChgFromTo'] = (new_lc * 100) + new_lc

        # Check for leapfrogging. The code does not prevent this.
        print('Final checks...')
        for plot in np.unique(df[df.MapChg.values].plotid):
            masked_arr = df[(df.plotid == plot) & (df.MapChgYear > 0)].MapChgYear.values
            if not all(masked_arr[i] <= masked_arr[i + 1] for i in range(len(masked_arr) - 1)):
                raise Exception('Warning! Leapfrog change year in plot: {}'.format(plot))

    # Switch from True/False values to strings for clarity
    chg = {True: 'Chg', False: 'NoChg'}
    df['RefChg'] = df.RefChg.apply(lambda x: chg[x])
    df['MapChg'] = df.MapChg.apply(lambda x: chg[x])

    # Get rid of the invalid data points, those don't count for change or no-change.
    df.drop(df[~df.Valid].index, inplace=True)

    return df
